<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>ConvNetJS Trainer Comparison on MNIST</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <link href="./styles.css" rel="stylesheet">

<style></style>

<script src="../shared/jquery-1.8.3.min.js"></script>
<script src="../../build/convnet.js"></script>
<script src="../shared/mnist_labels.js"></script>
<script src="./script.js"></script>

</head>
<body>
  <div id="wrap">
  <h2 style="text-align: center;"><a href="http://cs.stanford.edu/people/karpathy/convnetjs/">ConvNetJS</a> Trainer demo on MNIST</h2>
  <h1>Description</h1>
  <p>
    This demo lets you evaluate multiple trainers against each other on MNIST. By default I've set up a little benchmark that puts SGD/SGD with momentum/Adagrad/Adadelta/Nesterov against each other. For reference math and explanations on these refer to Matthew Zeiler's <a href="http://www.matthewzeiler.com/pubs/googleTR2012/googleTR2012.pdf">Adadelta paper</a> (Windowgrad is Idea #1 in the paper). In my own experience, Adagrad/Adadelta are "safer" because they don't depend so strongly on setting of learning rates (with Adadelta being slightly better), but well-tuned SGD+Momentum almost always converges faster and at better final values.
  </p>
  <p>Report questions/bugs/suggestions to <a href="https://twitter.com/karpathy">@karpathy</a>.</p>

  <textarea id="layerdef" style="width:100%; height:400px;">
  </textarea>
  <br /><br  />
  <input id="buttontp" type="submit" value="re-run" onclick="reload();" style="width: 300px; height: 50px;"/>

  <h1>Loss vs. Number of examples seen</h1>
  <canvas id="lossgraph" width="800" height="400"></canvas>

  <h1>Testing Accuracy vs. Number of examples seen</h1>
  <canvas id="testaccgraph" width="800" height="400"></canvas>

  <h1>Training Accuracy vs. Number of examples seen</h1>
  <canvas id="trainaccgraph" width="800" height="400"></canvas>

  </div>
</body>
</html>